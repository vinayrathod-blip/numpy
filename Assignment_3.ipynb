{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f136799-d115-4322-abf4-8b4193633a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is BERT and how does it work?\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained Transformer-based model.\n",
    "It reads text bidirectionally (left + right context together), making it great for understanding meaning. \n",
    "It is trained with Masked Language Modeling (MLM) and Next Sentence Prediction (NSP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f6b9df-3893-40b1-8676-b1c52fd4a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What are the main advantages of using the attention mechanism in neural networks?\n",
    "\n",
    "\n",
    "Captures long-range dependencies better than RNNs\n",
    "\n",
    "Focuses on important words/tokens\n",
    "\n",
    "Works in parallel, faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36ca028-262b-43c4-9f84-60addd1c0b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. How does the self-attention mechanism differ from traditional attention mechanisms?\n",
    "\n",
    "Traditional: attends to encoder hidden states (Seq2Seq).\n",
    "\n",
    "Self-attention: each token attends to all tokens in the same sequence (within encoder or decoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a04ce6f-b1d4-435a-aacf-96aa4d0d6565",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. What is the role of the decoder in a Seq2Seq model?\n",
    "\n",
    "The decoder generates the output sequence step by step. It takes the encoder’s context\n",
    "(or attention over encoder outputs) plus its previously generated tokens to predict the next token.\n",
    "It works autoregressively until the sequence ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36add1-ba77-407b-85cf-9d9750afd174",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What is the difference between GPT-2 and BERT models?\n",
    "\n",
    "BERT: Bidirectional, trained with Masked Language Modeling (MLM) and Next Sentence Prediction (NSP),\n",
    "designed for understanding tasks (classification, Q&A).\n",
    "\n",
    "GPT-2: Autoregressive, unidirectional (left-to-right), trained for generation tasks\n",
    "(next-word prediction, text completion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b668b6-4ff1-4725-95ac-e38925686aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "6.Why is the Transformer model considered more efficient than RNNs and LSTMs?\n",
    "\n",
    "Fully parallelizable (RNNs process sequentially).\n",
    "\n",
    "Captures long-range dependencies directly with attention.\n",
    "\n",
    "Faster training using GPUs/TPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb68c6e-a4d4-46c3-9b9f-0c4a283cd6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "7.Explain how the attention mechanism works in a Transformer model.\n",
    "\n",
    "\n",
    "\n",
    "Each token forms Query (Q), Key (K), and Value (V) vectors.\n",
    "\n",
    "Compute similarity between Query and Keys -> gives attention scores.\n",
    "\n",
    "Apply softmax -> weights.\n",
    "\n",
    "Weighted sum of Values -> updated representation of the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c26c5-1b4d-4821-b9f5-5785f41c20d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "8.What is the difference between an encoder and a decoder in a Seq2Seq model?\n",
    "\n",
    "Encoder: Reads input sequence -> produces a context representation.\n",
    "\n",
    "Decoder: Uses that context (and attention) to generate the output sequence step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78d83ab-a05f-43d7-a9fb-6ca9a904725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "9.What is the primary purpose of using the self-attention mechanism in transformers?\n",
    "\n",
    "To let each token attend to all other tokens in the same sequence,\n",
    "capturing contextual relationships efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d95f03-8509-45cf-816e-744b4f6c5e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "10.How does the GPT-2 model generate text?\n",
    "\n",
    "It generates tokens autoregressively:\n",
    "\n",
    "Predicts the next token from the context.\n",
    "\n",
    "Appends it to the input.\n",
    "\n",
    "Repeats until reaching end of sequence or stop condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336164a2-1424-4666-a86d-c5be8c27fc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "11.What is the main difference between the encoder-decoder architecture and a simple neural network?\n",
    "\n",
    "Encoder-decoder: Handles variable-length input and output sequences (e.g., translation).\n",
    "\n",
    "Simple NN: Works on fixed-size inputs and outputs, no sequence handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d047fd-049f-4fbf-a726-d9c34d151608",
   "metadata": {},
   "outputs": [],
   "source": [
    "12.Explain the concept of “fine-tuning” in BERT.\n",
    "\n",
    "Fine-tuning = taking pre-trained BERT weights and adapting them to a specific task by adding a small output\n",
    "layer (e.g., classifier) and training on a labeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41e7823-afef-41bf-8d6a-85e3a723a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "13.How does the attention mechanism handle long-range dependencies in sequences?\n",
    "\n",
    "Every token directly attends to all others with weighted scores. Unlike RNNs, \n",
    "it doesn’t rely on passing information step by step,so dependencies between far-apart tokens are captured \n",
    "easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7078041c-c10f-45a5-90bc-556363f2f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "14.What is the core principle behind the Transformer architecture?\n",
    "\n",
    "Replace recurrence with self-attention and parallelization for efficiency and scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35abc198-7a08-46da-9444-ea05bdca1f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "15.What is the role of the position encoding in a Transformer model?\n",
    "\n",
    "Since Transformers don’t have recurrence, positional encoding adds order information\n",
    "(which token comes first, second, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45804133-1482-4392-8fec-fa279b4e18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "16.How do Transformers use multiple layers of attention?\n",
    "\n",
    "They stack multiple self-attention layers, where each layer learns different aspects of relationships \n",
    "(syntax in lower layers, semantics in higher layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fa948c-b0b7-4886-8f54-ac1e4f806d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "17.What does it mean when a model is described as “autoregressive” like GPT-2?\n",
    "\n",
    "It generates text token by token, predicting the next token using only past context (not future words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b8e4c7-8524-42e8-a618-ed49e67aaccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "18.How does BERT's bidirectional training improve its performance?\n",
    "\n",
    "BERT sees both left and right context when predicting words, leading to deeper understanding and\n",
    "better performance on comprehension tasks (Q&A, sentiment, classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac112656-e036-4d0c-aebb-034deb80f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "19.What are the advantages of using the Transformer over RNN-based models in NLP?\n",
    "\n",
    "Faster training (parallel processing).\n",
    "\n",
    "Handles long-range context better.\n",
    "\n",
    "Achieves state-of-the-art results on many NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b968c-bf6b-4401-b835-2b5fe445fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "20.What is the attention mechanism’s impact on the performance of models like BERT and GPT-2?\n",
    "\n",
    "It enables deep contextual understanding and scalable learning, making these models powerful\n",
    "    for both understanding and generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07578d56-e816-4101-95da-dbcfa2e8ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90948fe6-ec39-4a9f-b277-becd377ce536",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.How to implement a simple text classification model using LSTM in Keras?\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=100),\n",
    "    LSTM(128),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e640dc0d-2373-4710-b265-826737f513a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "2.How to generate sequences of text using a Recurrent Neural Network (RNN)?\n",
    "\n",
    "Train an RNN (LSTM/GRU) on a text corpus for next-character or next-word prediction.\n",
    "At inference:\n",
    "\n",
    "Give a seed text.\n",
    "\n",
    "Predict next token.\n",
    "\n",
    "Append prediction to seed.\n",
    "\n",
    "Repeat until desired length.\n",
    "\n",
    "\n",
    "seed = \"Once upon\"\n",
    "for i in range(50):\n",
    "    pred = model.predict(seed)   # predict next token\n",
    "    seed += pred                 # append prediction\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d355acf7-acb9-430e-a023-36bb289428a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.How to perform sentiment analysis using a simple CNN model?\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(10000, 128, input_length=100),\n",
    "    Conv1D(128, 5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9a18a8-095e-40d7-9bf7-ade735056a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "4.How to perform Named Entity Recognition (NER) using spacy?\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c072da6b-d983-42a8-a538-1f38c1767b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "5.How to implement a simple Seq2Seq model for machine translation using LSTM in Keras?\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "# Encoder\n",
    "enc_inputs = Input(shape=(None,))\n",
    "enc_lstm = LSTM(256, return_state=True)\n",
    "enc_outputs, state_h, state_c = enc_lstm(enc_inputs)\n",
    "enc_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "dec_inputs = Input(shape=(None,))\n",
    "dec_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "dec_outputs, _, _ = dec_lstm(dec_inputs, initial_state=enc_states)\n",
    "dec_dense = Dense(vocab_size, activation='softmax')\n",
    "dec_outputs = dec_dense(dec_outputs)\n",
    "\n",
    "model = Model([enc_inputs, dec_inputs], dec_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89218cb5-8a0b-4bf6-8dae-f1eae5f02cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "6.How to generate text using a pre-trained transformer model (GPT-2)?\n",
    "\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "input_ids = tokenizer.encode(\"Once upon a time\", return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "\n",
    "print(tokenizer.decode(output[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5d65b7-5f70-4169-9580-97802f0dcede",
   "metadata": {},
   "outputs": [],
   "source": [
    "7.How to apply data augmentation for text in NLP?\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "aug = naw.SynonymAug(aug_p=0.3)\n",
    "augmented_text = aug.augment(\"I love natural language processing\")\n",
    "print(augmented_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a09468-9779-4cf9-b8b2-906f27693840",
   "metadata": {},
   "outputs": [],
   "source": [
    "8.How can you add an Attention Mechanism to a Seq2Seq model?\n",
    "\n",
    "\n",
    "At each decoder step, compute attention weights over encoder states.\n",
    "\n",
    "Use weighted sum (context vector) along with decoder state.\n",
    "\n",
    "Improves alignment in translation tasks.\n",
    "# _________________________________________________________________________________________________________#\n",
    "score = dot(DecoderHidden, EncoderOutputs)   # alignment scores\n",
    "weights = softmax(score)                      # attention weights\n",
    "context = sum(weights * EncoderOutputs)       # context vector\n",
    "DecoderInput = concat(context, prev_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6524465-4d5e-4d0d-876f-b901fd20b4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
