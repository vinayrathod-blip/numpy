{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de10b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: What is the main purpose of RCNN in object detection?\n",
    "A: The main purpose of RCNN (Region-based Convolutional Neural Network) is to \n",
    "    detect objects in images by first generating region proposals \n",
    "    (candidate object locations), then classifying each region and refining\n",
    "    bounding boxes for precise object localization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98153aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2: What is the difference between Fast RCNN and Faster RCNN?\n",
    "A: Fast RCNN improves over RCNN by processing the entire image with a CNN only \n",
    "once and then using Region of Interest (RoI) pooling to extract features for \n",
    "proposals. Faster RCNN further improves speed by replacing the external region \n",
    "proposal step with a Region Proposal Network (RPN) that shares convolutional\n",
    "features, making the detection pipeline end-to-end and faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d724dd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3: How does YOLO handle object detection in real-time?\n",
    "A: YOLO (You Only Look Once) divides the image into a grid and predicts \n",
    "bounding boxes and class probabilities directly from the full image in a single\n",
    "forward pass of the network,enabling fast, real-time detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf4b5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4: Explain the concept of Region Proposal Networks (RPN) in Faster RCNN.\n",
    "A: RPN is a fully convolutional network that predicts object proposals by \n",
    "sliding over the feature map. It outputs bounding box coordinates and \n",
    "objectness scores, sharing convolutional features with the detection network, \n",
    "which speeds up proposal generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69deba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5: How does YOLOv9 improve upon its predecessors?\n",
    "A: YOLOv9 improves by incorporating more efficient architectures, \n",
    "better backbone networks, improved feature fusion techniques, \n",
    "enhanced anchor box design, and optimized training strategies \n",
    "to increase accuracy and speed compared to earlier YOLO versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0b8dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6: What role does non-max suppression play in YOLO object detection?\n",
    "A: Non-max suppression (NMS) removes duplicate bounding boxes by selecting \n",
    "the box with the highest confidence score and eliminating others with high\n",
    "overlap, ensuring each object is detected only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606470d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7: Describe the data preparation process for training YOLOv9.\n",
    "A: Data preparation involves collecting labeled images, annotating objects\n",
    "with bounding boxes and class labels, resizing images to a consistent \n",
    "input size, applying data augmentation techniques (like flipping, scaling, \n",
    "color adjustments), and formatting annotations to YOLO's required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0135235",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8: What is the significance of anchor boxes in object detection models like\n",
    "    YOLOv9?\n",
    "    \n",
    "A: Anchor boxes serve as predefined templates of different aspect ratios\n",
    "and scales that help the model predict bounding boxes relative to them, \n",
    "improving localization accuracy and handling objects of varying sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9: What is the key difference between YOLO and R-CNN architectures?\n",
    "A: YOLO treats object detection as a single regression problem and predicts \n",
    "bounding boxes and class probabilities directly from the whole image in one \n",
    "step,while R-CNN methods generate region proposals first and then classify\n",
    "each region separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235158e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10: Why is Faster RCNN considered faster than Fast RCNN?\n",
    "A: Faster RCNN replaces the slow external region proposal method \n",
    "    (selective search) used in Fast RCNN with an efficient \n",
    "    Region Proposal Network (RPN) integrated into the CNN, \n",
    "    speeding up the proposal generation and overall detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658fd235",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q11: What is the role of selective search in RCNN?\n",
    "A: Selective search generates candidate object regions (region proposals) by \n",
    "grouping similar pixels based on color, \n",
    "texture, size, and shape, providing likely areas for object detection in RCNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1399363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q12: How does YOLOv9 handle multiple classes in object detection?\n",
    "A: YOLOv9 predicts class probabilities for each bounding box across all \n",
    "classes simultaneously using a multi-label classification approach,\n",
    "allowing it to detect multiple classes in one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b55287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q13: What are the key differences between YOLOv3 and YOLOv9?\n",
    "A: YOLOv9 includes architectural improvements like more efficient backbones,\n",
    "better feature pyramid networks, refined anchor box strategies, and\n",
    "optimized training methods,resulting in higher accuracy and faster \n",
    "inference compared to YOLOv3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8229cf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q14: How is the loss function calculated in Faster RCNN?\n",
    "A: The loss function combines classification loss \n",
    "(cross-entropy for object class prediction) and bounding box regression loss\n",
    "(smooth L1 loss) from both the RPN and the final detection network, \n",
    "optimized jointly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abceb217",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q15: Explain how YOLOv9 improves speed compared to earlier versions.\n",
    "A: YOLOv9 uses optimized backbone architectures, lightweight feature \n",
    "    extraction modules, efficient anchor box matching, and streamlined \n",
    "    detection heads to reduce computation while maintaining accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b5b61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q16 What are some challenges faced in training YOLOv9?\n",
    "A: Challenges include balancing speed and accuracy, \n",
    "    handling diverse object sizes, managing class imbalance, \n",
    "    tuning anchor boxes, and ensuring effective data augmentation to \n",
    "    prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2c6992",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q17: How does the YOLOv9 architecture handle large and small object detection?\n",
    "A: YOLOv9 employs multi-scale feature maps and feature pyramid networks to \n",
    "    capture both high-level semantic \n",
    "    information for large objects and fine-grained details for small objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ed2743",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q18: What is the significance of fine-tuning in YOLO?\n",
    "A: Fine-tuning adapts a pre-trained YOLO model to a specific dataset or \n",
    "    domain by training it further on new data, improving accuracy and\n",
    "    reducing training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d75ce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q19: What is the concept of bounding box regression in Faster RCNN?\n",
    "A: Bounding box regression adjusts the coordinates of proposed regions\n",
    "    to better fit the ground truth objects by learning offsets, \n",
    "    improving localization accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3d57d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q20: Describe how transfer learning is used in YOLO.\n",
    "A: Transfer learning involves using a YOLO model pre-trained on a large dataset\n",
    "    (e.g., COCO) as a starting point, then fine-tuning it on a smaller, \n",
    "    task-specific dataset to leverage learned features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcaff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q21: What is the role of the backbone network in object detection models \n",
    "    like YOLOv9?\n",
    "A: The backbone extracts rich hierarchical features from input images,\n",
    "    serving as the base for detecting objects at multiple scales and locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a5395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q22: How does YOLO handle overlapping objects?\n",
    "A: YOLO predicts multiple bounding boxes per grid cell and \n",
    "   uses non-max suppression to resolve overlapping detections, \n",
    "   ensuring distinct objects are identified separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a60463",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q23: What is the importance of data augmentation in object detection?\n",
    "A: Data augmentation artificially increases dataset diversity by applying \n",
    "    transformations (e.g., rotation, scaling), helping models generalize\n",
    "    better and reducing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df700d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q24: How is performance evaluated in YOLO-based object detection?\n",
    "A: Performance is commonly evaluated using metrics like mean Average Precision\n",
    "    (mAP), Intersection over Union (IoU), precision, recall,\n",
    "    and inference speed (FPS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e9fd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q25: How do the computational requirements of Faster RCNN compare to those of YOLO?\n",
    "A: Faster RCNN generally requires more computation due to its two-stage pipeline \n",
    "    (proposal generation + classification),while YOLO is more lightweight and\n",
    "    optimized for faster, real-time detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce9dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q26: What role do convolutional layers play in object detection \n",
    "    with RCNN?\n",
    "A: Convolutional layers extract spatial and semantic features from images, \n",
    "    enabling the network to identify object patterns and locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c6c8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q27: How does the loss function in YOLO differ from other object detection models?\n",
    "A: YOLO's loss function combines coordinate loss, confidence loss, and \n",
    "    classification loss in a single unified function optimized end-to-end,\n",
    "    differing from multi-stage losses in models like Faster RCNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d66e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q28: What are the key advantages of using YOLO for real-time object detection?\n",
    "A: YOLO offers fast inference speeds, single-stage detection, and high accuracy,\n",
    "    making it suitable for real-time applications like video surveillance and \n",
    "    autonomous driving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a8818",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q29: How does Faster RCNN handle the trade-off between accuracy and speed?\n",
    "A: Faster RCNN balances accuracy and speed by integrating RPN for efficient \n",
    "    proposal generation,but still trades some speed for improved detection\n",
    "    accuracy compared to single-stage detectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8ef485",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q30: What is the role of the backbone network in both YOLO and Faster RCNN,\n",
    "    and how do they differ?\n",
    "A: Both use backbone networks to extract features, but YOLO's backbone is \n",
    "   typically optimized for speed and efficiency, whereas Faster RCNN's backbone\n",
    "    prioritizes feature richness for high detection accuracy. \n",
    "    The architectures of the backbones may differ accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e00753",
   "metadata": {},
   "source": [
    "### Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad58c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. How do you load and run inference on a custom image using the YOLOv8 model\n",
    "(labeled as YOLOv9)?\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8/YOLOv9 model (assuming ultralytics YOLO package)\n",
    "model = YOLO('yolov8n.pt')  \n",
    "\n",
    "# Load and run inference on a custom image\n",
    "results = model('path/to/your/image.jpg')\n",
    "\n",
    "# Print detected objects\n",
    "results.print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c046d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. How do you load the Faster RCNN model with a ResNet50 backbone \n",
    "  and print its architecture?\n",
    "    \n",
    "import torchvision.models.detection as detection\n",
    "import torchvision\n",
    "\n",
    "# Load Faster RCNN with ResNet50 backbone pretrained on COCO\n",
    "model = detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Print model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632e252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. How do you perform inference on an online image using the Faster RCNN model \n",
    "   and print the predictions?\n",
    "    \n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Load Faster RCNN model\n",
    "model = detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Load image from URL\n",
    "url = \"https://example.com/image.jpg\"\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "# Preprocess\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "img_tensor = transform(img)\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    prediction = model([img_tensor])\n",
    "\n",
    "print(prediction)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e53b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. How do you load an image and perform inference using YOLOv9, then\n",
    "display the detected objects with bounding boxes and class labels?\n",
    "\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8n.pt')\n",
    "results = model('path/to/image.jpg')\n",
    "\n",
    "# Display results with bounding boxes and labels\n",
    "results.show()  # Opens a window with detection\n",
    "\n",
    "# Or save the output image with detections\n",
    "results.save('output_folder/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aaa9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. How do you display bounding boxes for the detected objects in an image using Faster RCNN?\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Assuming `img` is a PIL image and `prediction` is output from Faster RCNN\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(img)\n",
    "\n",
    "boxes = prediction[0]['boxes'].cpu().numpy()\n",
    "labels = prediction[0]['labels'].cpu().numpy()\n",
    "scores = prediction[0]['scores'].cpu().numpy()\n",
    "\n",
    "for box, label, score in zip(boxes, labels, scores):\n",
    "    if score > 0.5:  # Threshold\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1, f'{label}: {score:.2f}', color='yellow', fontsize=12, backgroundcolor='black')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "871ea033",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. How do you perform inference on a local image using Faster RCNN?\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Load image locally and run inference (similar to question 3, just loading locally)\n",
    "\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "\n",
    "img = Image.open('path/to/local/image.jpg').convert(\"RGB\")\n",
    "img_tensor = transform(img)\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = model([img_tensor])\n",
    "\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88030160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `predictions` not found.\n"
     ]
    }
   ],
   "source": [
    "7. How can you change the confidence threshold for YOLO object detection and filter out low-confidence predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c3c9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. How can you change the confidence threshold for YOLO object detection and filter out low-confidence predictions\n",
    "\n",
    "\n",
    "# Run inference with confidence threshold filter\n",
    "results = model('path/to/image.jpg', conf=0.5)  # Confidence threshold set to 0.5\n",
    "\n",
    "# Filtered results will only show predictions above this confidence\n",
    "results.print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d56685",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. How do you plot the training and validation loss curves for model evaluation?\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have training logs stored as lists or from a training library\n",
    "train_losses = [ ... ]  # Your recorded training loss values\n",
    "val_losses = [ ... ]    # Your recorded validation loss values\n",
    "\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa491f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. How do you perform inference on multiple images from a local folder using Faster RCNN and display the bounding boxes for each?\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "folder_path = 'path/to/folder'\n",
    "image_files = [f for f in os.listdir(folder_path) if f.endswith(('jpg', 'png'))]\n",
    "\n",
    "for image_file in image_files:\n",
    "    img_path = os.path.join(folder_path, image_file)\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_tensor = transform(img)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prediction = model([img_tensor])\n",
    "    \n",
    "    # Display bounding boxes (use code from #5)\n",
    "    # ... (plotting code here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db59fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. How do you visualize the confidence scores alongside the bounding boxes for detected objects using Faster RCNN?\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "import torchvision.models.detection as detection\n",
    "\n",
    "# Load model and set to eval mode\n",
    "model = detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Load and preprocess image\n",
    "img_path = 'path/to/your/image.jpg'\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "img_tensor = transform(img)\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    prediction = model([img_tensor])\n",
    "\n",
    "# Extract results\n",
    "boxes = prediction[0]['boxes'].cpu().numpy()\n",
    "scores = prediction[0]['scores'].cpu().numpy()\n",
    "labels = prediction[0]['labels'].cpu().numpy()\n",
    "\n",
    "# Plot image and bounding boxes with confidence scores\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(img)\n",
    "\n",
    "for box, score, label in zip(boxes, scores, labels):\n",
    "    if score > 0.5:  # confidence threshold to filter predictions\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1, f'{label}: {score:.2f}', color='yellow', fontsize=10, backgroundcolor='black')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec9c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "11. How can you save the inference results (with bounding boxes) as a new image after performing detection using YOLO?\n",
    "\n",
    "\n",
    "results = model('path/to/image.jpg')\n",
    "\n",
    "# Save results image with bounding boxes drawn\n",
    "results.save('output_folder/')  # Saves images with detections to this folder\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
