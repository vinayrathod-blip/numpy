{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b217d363",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " 1. What is Generative AI?  \n",
    "Generative AI refers to a category of artificial intelligence systems designed to create new content such as text, images, music, or data by learning patterns from existing datasets.\n",
    "\n",
    "---\n",
    "\n",
    " 2. How is Generative AI different from traditional AI?  \n",
    "Traditional AI often focuses on classification or prediction based on input data, while Generative AI creates new, original content by modeling data distributions.\n",
    "\n",
    "---\n",
    "\n",
    " 3. Name two applications of Generative AI in the industry  \n",
    "1. Text generation for chatbots and content creation.  \n",
    "2. Image synthesis for design, art, and media production.\n",
    "\n",
    "---\n",
    "\n",
    " 4. What are some challenges associated with Generative AI?  \n",
    "Challenges include data bias, generating realistic but misleading content (deepfakes), high computational costs, and difficulty in evaluating generated outputs.\n",
    "\n",
    "---\n",
    "\n",
    " 5. Why is Generative AI important for modern applications?  \n",
    "It enables automation of creative tasks, personalized content generation, simulation for training, and augmentation of human creativity.\n",
    "\n",
    "---\n",
    "\n",
    " 6. What is probabilistic modeling in the context of Generative AI?  \n",
    "Probabilistic modeling involves creating models that represent the probability distributions of data, enabling the generation of new samples based on learned probabilities.\n",
    "\n",
    "---\n",
    "\n",
    " 7. Define a generative model  \n",
    "A generative model is a type of model that learns to generate new data samples similar to the training data by capturing its underlying distribution.\n",
    "\n",
    "---\n",
    "\n",
    " 8. Explain how an n-gram model works in text generation  \n",
    "An n-gram model predicts the next word in a sequence based on the previous (n-1) words by estimating the conditional probability of word sequences from training data.\n",
    "\n",
    "---\n",
    "\n",
    " 9. What are the limitations of n-gram models?  \n",
    "They suffer from data sparsity, limited context (short memory), and inability to capture long-range dependencies in language.\n",
    "\n",
    "---\n",
    "\n",
    " 10. How can you improve the performance of an n-gram model?  \n",
    "By applying smoothing techniques, increasing the size of n, and using back-off or interpolation methods to better estimate probabilities for unseen sequences.\n",
    "\n",
    "---\n",
    "\n",
    " 11. What is the Markov assumption, and how does it apply to text generation?  \n",
    "The Markov assumption states that the probability of a word depends only on a fixed number of previous words, simplifying text modeling by limiting context.\n",
    "\n",
    "---\n",
    "\n",
    " 12. Why are probabilistic models important in generative AI?  \n",
    "They allow the model to quantify uncertainty and generate diverse, plausible samples by modeling the distribution of data.\n",
    "\n",
    "---\n",
    "\n",
    " 13. What is an autoencoder?  \n",
    "An autoencoder is a neural network that learns to compress input data into a lower-dimensional latent representation and then reconstructs the original data from it.\n",
    "\n",
    "---\n",
    "\n",
    " 14. How does a VAE differ from a standard autoencoder?  \n",
    "A Variational Autoencoder (VAE) models the latent space probabilistically, learning distributions rather than fixed codes, enabling controlled and diverse generation.\n",
    "\n",
    "---\n",
    "\n",
    " 15. Why are VAEs useful in generative modeling?  \n",
    "They allow smooth interpolation in the latent space and can generate new data samples by sampling from learned latent distributions.\n",
    "\n",
    "---\n",
    "\n",
    " 16. What role does the decoder play in an autoencoder?  \n",
    "The decoder reconstructs the original data from the compressed latent representation.\n",
    "\n",
    "---\n",
    "\n",
    " 17. How does the latent space affect text generation in a VAE?  \n",
    "The latent space captures meaningful variations in the data; sampling from it enables generation of diverse and coherent text outputs.\n",
    "\n",
    "---\n",
    "\n",
    " 18. What is the purpose of the Kullback-Leibler (KL) divergence term in VAEs?  \n",
    "KL divergence regularizes the latent distribution to be close to a prior distribution (usually Gaussian), encouraging smooth and structured latent spaces.\n",
    "\n",
    "---\n",
    "\n",
    " 19. How can you prevent overfitting in a VAE?  \n",
    "By using regularization (like the KL term), dropout, early stopping, and ensuring a balanced capacity of the latent space.\n",
    "\n",
    "---\n",
    "\n",
    " 20. Explain why VAEs are commonly used for unsupervised learning tasks  \n",
    "Because they learn to represent data structure without labeled data, making them ideal for feature learning and generation.\n",
    "\n",
    "---\n",
    "\n",
    " 21. What is a transformer model?  \n",
    "A transformer is a neural network architecture that uses self-attention mechanisms to model dependencies in sequential data, excelling in tasks like language understanding and generation.\n",
    "\n",
    "---\n",
    "\n",
    " 22. Explain the purpose of self-attention in transformers  \n",
    "Self-attention allows the model to weigh the importance of different words in a sequence, capturing relationships regardless of distance.\n",
    "\n",
    "---\n",
    "\n",
    " 23. How does a GPT model generate text?  \n",
    "GPT generates text autoregressively by predicting the next word based on the sequence of previously generated words.\n",
    "\n",
    "---\n",
    "\n",
    " 24. What are the key differences between a GPT model and an RNN?  \n",
    "GPT uses transformer architecture with self-attention enabling parallel processing and long-range dependencies, whereas RNNs process sequentially with limited memory.\n",
    "\n",
    "---\n",
    "\n",
    " 25. How does fine-tuning improve a pre-trained GPT model?  \n",
    "Fine-tuning adapts the model to a specific domain or task by training it on targeted data, improving relevance and accuracy.\n",
    "\n",
    "---\n",
    "\n",
    " 26. What is zero-shot learning in the context of GPT models?  \n",
    "Zero-shot learning refers to GPT's ability to perform tasks without explicit training on them, based solely on prompts and pre-trained knowledge.\n",
    "\n",
    "---\n",
    "\n",
    " 27. Describe how prompt engineering can impact GPT model performance  \n",
    "Carefully crafted prompts guide GPT to generate more accurate, relevant, or creative responses by shaping the input context.\n",
    "\n",
    "---\n",
    "\n",
    " 28. Why are large datasets essential for training GPT models?  \n",
    "Large datasets provide diverse language patterns, improving the model's generalization and robustness.\n",
    "\n",
    "---\n",
    "\n",
    " 29. What are potential ethical concerns with GPT models?  \n",
    "Risks include generation of biased, harmful, or misleading content, privacy issues, and misuse for misinformation.\n",
    "\n",
    "---\n",
    "\n",
    " 30. How does the attention mechanism contribute to GPTâ€™s ability to handle long-range dependencies?  \n",
    "Attention allows GPT to focus on relevant parts of the entire input sequence, capturing relationships even between distant words.\n",
    "\n",
    "---\n",
    "\n",
    " 31. What are some limitations of GPT models for real-world applications?  \n",
    "They can produce incorrect or nonsensical outputs, struggle with reasoning, require large compute resources, and may perpetuate biases.\n",
    "\n",
    "---\n",
    "\n",
    " 32. How can GPT models be adapted for domain-specific text generation?  \n",
    "By fine-tuning on domain-specific corpora or using prompt engineering to steer output style and content.\n",
    "\n",
    "---\n",
    "\n",
    " 33. What are some common metrics for evaluating text generation quality?  \n",
    "Metrics include BLEU, ROUGE, perplexity, and human evaluation for fluency and relevance.\n",
    "\n",
    "---\n",
    "\n",
    " 34. Explain the difference between deterministic and probabilistic text generation  \n",
    "Deterministic generation produces the same output for a given input, while probabilistic generation samples from a distribution, yielding varied outputs.\n",
    "\n",
    "---\n",
    "\n",
    " 35. How does beam search improve text generation in language models?  \n",
    "Beam search explores multiple candidate sequences simultaneously, selecting the most likely to improve overall output quality.\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e197940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate a random sentence using probabilistic modeling (Markov Chain)\n",
    "import random\n",
    "\n",
    "def generate_markov_chain(text, n=2, length=10):\n",
    "    words = text.split()\n",
    "    ngrams = {}\n",
    "    for i in range(len(words)-n):\n",
    "        key = tuple(words[i:i+n])\n",
    "        next_word = words[i+n]\n",
    "        if key not in ngrams:\n",
    "            ngrams[key] = []\n",
    "        ngrams[key].append(next_word)\n",
    "    current = random.choice(list(ngrams.keys()))\n",
    "    output = list(current)\n",
    "    for _ in range(length - n):\n",
    "        if current in ngrams:\n",
    "            next_word = random.choice(ngrams[current])\n",
    "            output.append(next_word)\n",
    "            current = tuple(output[-n:])\n",
    "        else:\n",
    "            break\n",
    "    return ' '.join(output)\n",
    "\n",
    "sample_text = \"The cat is on the mat\"\n",
    "print(\"Markov Chain generated sentence:\")\n",
    "print(generate_markov_chain(sample_text, n=2, length=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c12b2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Build a simple Autoencoder model using Keras for sentences (example using character-level encoding)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Example simple character set and sentence for demo\n",
    "chars = sorted(list(set(\"The cat is on the mat\")))\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = {i: c for c, i in char_to_int.items()}\n",
    "max_len = 20  # max length of sequence\n",
    "\n",
    "def encode_sentence(sentence):\n",
    "    encoded = np.zeros((max_len, len(chars)))\n",
    "    for i, c in enumerate(sentence[:max_len]):\n",
    "        encoded[i, char_to_int[c]] = 1\n",
    "    return encoded.flatten()\n",
    "\n",
    "# Prepare training data (repeat for demo)\n",
    "sentences = [\"The cat is on the mat\"] * 100\n",
    "x_train = np.array([encode_sentence(s) for s in sentences])\n",
    "\n",
    "input_dim = x_train.shape[1]\n",
    "encoding_dim = 32\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.fit(x_train, x_train, epochs=20, batch_size=10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2776d057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Use Hugging Face transformers library to fine-tune GPT-2 on custom data (template, requires datasets and tokenizers)\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "def fine_tune_gpt2(train_file, output_dir):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "    dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=train_file,\n",
    "        block_size=128)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=2,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "\n",
    "# To run, prepare a 'train.txt' file with your custom data\n",
    "# fine_tune_gpt2('train.txt', './gpt2-finetuned')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2d5c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Simple RNN Text Generation Model in Keras\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "texts = [\n",
    "    \"the cat is on the mat\",\n",
    "    \"the dog is in the house\",\n",
    "    \"the cat likes to sleep\",\n",
    "    \"the dog likes to play\"\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = []\n",
    "for line in texts:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_seq_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre'))\n",
    "\n",
    "X = input_sequences[:,:-1]\n",
    "y = input_sequences[:,-1]\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y = to_categorical(y, num_classes=total_words)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=max_seq_len-1))\n",
    "model.add(SimpleRNN(50))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(X, y, epochs=100, verbose=1)\n",
    "\n",
    "def generate_text(seed_text, next_words=5):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_word_index = np.argmax(predicted, axis=1)[0]\n",
    "        output_word = tokenizer.index_word.get(predicted_word_index, '')\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "print(generate_text(\"the cat\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586df7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. LSTM-based Text Generation Model in Keras\n",
    "\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=max_seq_len-1))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(X, y, epochs=100, verbose=1)\n",
    "\n",
    "def generate_text_lstm(seed_text, next_words=5):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_word_index = np.argmax(predicted, axis=1)[0]\n",
    "        output_word = tokenizer.index_word.get(predicted_word_index, '')\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "print(generate_text_lstm(\"the dog\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ddd59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. GPT-2 Story Generation with Hugging Face Transformers\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model_gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "def generate_story(prompt, max_length=100):\n",
    "    inputs = tokenizer_gpt2.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model_gpt2.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    story = tokenizer_gpt2.decode(outputs[0], skip_special_tokens=True)\n",
    "    return story\n",
    "\n",
    "prompt = \"Once upon a time in a land far away,\"\n",
    "print(generate_story(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6bc9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. GRU-based Text Generation Model in Keras\n",
    "\n",
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=max_seq_len-1))\n",
    "model.add(GRU(100))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(X, y, epochs=100, verbose=1)\n",
    "\n",
    "def generate_text_gru(seed_text, next_words=5):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_word_index = np.argmax(predicted, axis=1)[0]\n",
    "        output_word = tokenizer.index_word.get(predicted_word_index, '')\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "print(generate_text_gru(\"the cat\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b550c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. GPT-2 Text Generation with Beam Search Decoding\n",
    "\n",
    "def generate_text_beam_search(prompt, max_length=100, num_beams=5):\n",
    "    inputs = tokenizer_gpt2.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model_gpt2.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    text = tokenizer_gpt2.decode(outputs[0], skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "print(generate_text_beam_search(\"In a future world,\", max_length=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813a7e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. GPT-2 Text Generation with Custom Temperature Setting\n",
    "\n",
    "def generate_text_temperature(prompt, max_length=100, temperature=1.0):\n",
    "    inputs = tokenizer_gpt2.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model_gpt2.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    text = tokenizer_gpt2.decode(outputs[0], skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "print(generate_text_temperature(\"The mysterious forest\", temperature=0.7))\n",
    "print(generate_text_temperature(\"The mysterious forest\", temperature=1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70330678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. GPT-2 Temperature Sampling Experiment\n",
    "\n",
    "temperatures = [0.3, 0.7, 1.0, 1.3]\n",
    "prompt = \"The future of AI is\"\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"Temperature: {temp}\")\n",
    "    print(generate_text_temperature(prompt, temperature=temp))\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b86fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Simple LSTM Text Generation Model\n",
    "\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=max_seq_len-1))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(X, y, epochs=100, verbose=1)\n",
    "\n",
    "def generate_text_lstm(seed_text, next_words=5):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_word_index = np.argmax(predicted, axis=1)[0]\n",
    "        output_word = tokenizer.index_word.get(predicted_word_index, '')\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "print(generate_text_lstm(\"the dog\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba79dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Custom Attention-based Text Generation Architecture in Keras\n",
    "\n",
    "from tensorflow.keras.layers import Layer, Input, Dense, LSTM, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight',\n",
    "                                 shape=(input_shape[-1], 1),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias',\n",
    "                                 shape=(input_shape[1], 1),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "input_seq = Input(shape=(max_seq_len-1,))\n",
    "embedding = Embedding(total_words, 10)(input_seq)\n",
    "lstm_out = LSTM(100, return_sequences=True)(embedding)\n",
    "attention_out = Attention()(lstm_out)\n",
    "output = Dense(total_words, activation='softmax')(attention_out)\n",
    "\n",
    "model = Model(inputs=input_seq, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(X, y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d64596e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
