{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is deep learning, and how is it connected to artificial intelligence?\n",
        "\n",
        "Ans : Deep Learning is subset of machine learning.In Machine Learning we have parametric  and  non-parametric equation but in deep learning we have only parametric equations.Deep learning use in multilayer neural network"
      ],
      "metadata": {
        "id": "0eBR7qHb3x1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is a neural network, and what are the different types of neural networks?\n",
        "\n",
        "Ans : Neural Network is basically a network in which have a layers of  neurons.\n",
        "\n",
        "types of neural networks : Artificial Neural Network (ANN),Convolution Neural Network(CNN),Recurrent Neural Network(RNN)etc."
      ],
      "metadata": {
        "id": "qqZzbwYb4q6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the mathematical structure of a neural network?\n",
        "\n",
        "\n",
        "Ans : For a feedforward network with layers L.\n",
        "\n",
        "Each layer compute : W*X + B = Y\n",
        "Then Activation Function : f(Y)\n",
        "\n",
        "Input = X\n",
        "Output = Y\n",
        "\n",
        "Training Minimise Loss over dataset by updating W,b with gradients from backpropagation."
      ],
      "metadata": {
        "id": "Xon3W_Mp5elU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zflQ6yza6ywK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) What is an activation function, and why is it essential in neural networks\n",
        "\n",
        "Ans: An activation function applies a nonlinearity to each neuron’s weighted sum. It’s essential because without nonlinearity, the network reduces to a single linear transformation regardless of depth — nonlinearity allows the network to model complex functions."
      ],
      "metadata": {
        "id": "pQCfOxdd645C"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wGTUd-gf7C_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Could you list some common activation functions used in neural networks.\n",
        "Ans:\n",
        "\n",
        "Sigmoid\n",
        "\n",
        "tanh(x)\n",
        "\n",
        "ReLU\n",
        "\n",
        "Leaky ReLU\n",
        "\n",
        "Softmax"
      ],
      "metadata": {
        "id": "PV9jb2SV7HkS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IANJgaZ47gMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is a multilayer neural network\n",
        "\n",
        "Ans: A multilayer neural network  has one or more hidden layers between input and output; each layer is made of neurons with learnable weights and nonlinear activations."
      ],
      "metadata": {
        "id": "Rny7FAeI7pB-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fY3eQka27x3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.What is a loss function, and why is it crucial for neural network training\n",
        "\n",
        "Ans: A loss (cost) function  measures discrepancy between the model’s predictions and true targets. It is crucial because training optimizes  this function to make predictions accurate; gradients of the loss guide weight updates."
      ],
      "metadata": {
        "id": "NJ68NP9q72Th"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TCkhzxT879qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What are some common types of loss functions?\n",
        "\n",
        "Ans: Common losses:\n",
        "\n",
        "MSE (Mean Squared Error) — regression.\n",
        "\n",
        "MAE (Mean Absolute Error) — regression robust to outliers.\n",
        "\n",
        "Binary cross-entropy — binary classification.\n",
        "\n",
        "Categorical cross-entropy — multi-class classification.\n"
      ],
      "metadata": {
        "id": "1iD2cyVx8NAz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_3dxJBxY8Yxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. How does a neural network learn\n",
        "\n",
        "Ans:\n",
        "Learning = iterative optimization:\n",
        "\n",
        "Forward pass: compute predictions.\n",
        "\n",
        "Compute loss.\n",
        "\n",
        "Backward pass: compute gradients of loss w.r.t. weights (backpropagation).\n",
        "\n",
        "Update weights using an optimizer (e.g., SGD, Adam).\n",
        "Repeat over many epochs."
      ],
      "metadata": {
        "id": "3nFJhZtF8cbk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rzeSOWp78khI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is an optimizer in neural networks, and why is it necessary\n",
        "\n",
        "Answer: An optimizer is an algorithm that updates model parameters using gradients to minimize the loss. It’s necessary to transform gradient information into practical parameter updates (step sizes, momentum, adaptive learning rates) that lead to convergence."
      ],
      "metadata": {
        "id": "cXNxXM2b8n8E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xej3ev3j8xmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Could you briefly describe some common optimizers\n",
        "\n",
        "Ans:\n",
        "\n",
        "SGD — simple gradient descent (optionally with momentum).\n",
        "\n",
        "Momentum / Nesterov — accelerates SGD using velocity.\n",
        "\n",
        "Adam — adaptive learning rates using estimates of first & second moments; commonly used.\n",
        "\n",
        "RMSProp — adaptive per-parameter learning rates.\n",
        "\n",
        "Adagrad — per-parameter scaling"
      ],
      "metadata": {
        "id": "lG32Z3Tl83FB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v8g_8vrz9A6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12) Can you explain forward and backward propagation in a neural network\n",
        "\n",
        "Answer:\n",
        "\n",
        "Forward propagation: compute each layer’s linear transform Z then activation A  until get output y.\n",
        "\n",
        " Backward propagation (backprop): starting from dl/dy  apply chain rule layer-by-layer to compute gradients dl/dw , dl/db used by the optimizer to update parameters."
      ],
      "metadata": {
        "id": "-xeGuIXF9XTk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q9fJs7rn-DO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is weight initialization, and how does it impact training\n",
        "\n",
        "Ans:\n",
        "Weight initialization sets initial values W,b before training. Good initialization helps maintain stable activation and gradient magnitudes across layers, speeding convergence and avoiding vanishing/exploding gradients."
      ],
      "metadata": {
        "id": "YU_U8MND-T7M"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4x9CmAxJ-i8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the vanishing gradient problem in deep learning\n",
        "\n",
        "Answer: In deep networks, gradients can shrink exponentially layer-by-layer during backprop, making early layers learn very slowly . It often happens with saturating activations and poor initialization."
      ],
      "metadata": {
        "id": "hynOFxOx-lgc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C4lC4e0K-tX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is the exploding gradient problem.\n",
        "\n",
        "Ans\n",
        ": Opposite of vanishing: gradients grow exponentially during backprop and cause numeric overflow or unstable updates. Caused by large weights, poor initialization, or high learning rates."
      ],
      "metadata": {
        "id": "AEzR4Hye-xbx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3gW3T_kEx2CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practical"
      ],
      "metadata": {
        "id": "_8kHn9xAyAyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1 How do you create a simple perceptron for basic binary classification?\n",
        "import numpy as np\n",
        "X = np.array([[0,0],\n",
        "              [0,1],\n",
        "              [1,0],\n",
        "              [1,1]])\n",
        "y = np.array([0, 1, 1, 0])\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(1,input_dim=2,activation='sigmoid'))\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy',optimizer= 'sgd',metrics  = ['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(X,y,epochs = 50)\n",
        "predictions = model.predict(X)\n",
        "print(\"Predictions:\", predictions)\n",
        "\n",
        "\n",
        "loss,accuracy= model.evaluate(X,y)\n",
        "\n",
        "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s1qrC52xyCjU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. How can you build a neural network with one hidden layer using Keras?\n",
        "import numpy as np\n",
        "X = np.array([[0,0],\n",
        "              [0,1],\n",
        "              [1,0],\n",
        "              [1,1]])\n",
        "y = np.array([0, 1, 1, 0])\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(4,input_dim=2,activation='relu'))\n",
        "model.add(Dense(1,activation=\"sigmoid\"))\n",
        "model.compile(loss='binary_crossentropy',optimizer= 'adam',metrics=['accuracy'])\n",
        "\n",
        "model.fit(X,y,epochs=10,verbose=0)\n",
        "pred = model.predict(X)\n",
        "print(\"Predictions:\", pred)\n",
        "loss,acc = model.evaluate(X,y)\n",
        "print(f\"Accuracy: {acc*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "bXsWwkYr01ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. How do you initialize weights using the Xavier (Glorot) initialization method in Keras?\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.initializers import GlorotUniform, GlorotNormal\n",
        "\n",
        "# Create a simple model\n",
        "model = Sequential([\n",
        "    Dense(64, input_dim=100, kernel_initializer=GlorotUniform()),\n",
        "    Dense(32, kernel_initializer=GlorotNormal()),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n"
      ],
      "metadata": {
        "id": "AimdM07Q4lV7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. How can you apply different activation functions in a neural network in Keras?\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(32,input_dim = 20,activation='relu'),\n",
        "    Dense(20,activation='tanh'),\n",
        "    Dense(10,activation='sigmoid'),\n",
        "    Dense(10,activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "7M03oFWWWr2q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5.How do you add dropout to a neural network model to prevent overfitting?\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(26,input_dim=20,activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "SeETcvVaYKq5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6.How do you manually implement forward propagation in a simple neural network?\n",
        "\n",
        "import numpy as np\n",
        "X = np.array([[0.5, 0.2, 0.1]])\n",
        "\n",
        "W1 = np.array([[0.1, 0.4],\n",
        "               [0.2, 0.3],\n",
        "               [0.5, 0.9]])\n",
        "b1 = np.array([[0.1, 0.2]])\n",
        "\n",
        "W2 = np.array([[0.3],\n",
        "               [0.7]])\n",
        "b2 = np.array([[0.5]])\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "#Forward propagation\n",
        "z1 = np.dot(X, W1) + b1\n",
        "a1 = sigmoid(z1)\n",
        "\n",
        "z2 = np.dot(a1, W2) + b2\n",
        "a2 = sigmoid(z2)\n",
        "\n",
        "print(\"Hidden layer output (a1):\", a1)\n",
        "print(\"Final output (a2):\", a2)\n"
      ],
      "metadata": {
        "id": "UfmWtfIJY2TH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7. How do you add batch normalization to a neural network model in Keras?\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,BatchNormalization,Activation\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(62,input_dim = 30,use_bias=False),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "\n",
        "    Dense(34, use_bias=False),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "\n",
        "    Dense(10, activation='softmax')\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "8fCsr1iHZoPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8. How can you visualize the training process with accuracy and loss curves?\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(64, input_dim=100, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X,y,epochs=20, batch_size=32)\n",
        "\n",
        "# Plot training & validation accuracy and loss\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['accuracy'], label='train accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.plot(history.history['val_loss'], label='val loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "N-XYwdhdbVsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9.How can you use gradient clipping in Keras to control the gradient size and prevent exploding gradients?\n",
        "\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001, clipvalue=1.0)\n",
        "\n",
        "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------#\n",
        "#  clipnorm\n",
        "optimizer = Adam(learning_rate=0.001, clipnorm=1.0)\n",
        "\n",
        "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "wT1t-K07bzra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10. How can you create a custom loss function in Keras?\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import backend\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(64, input_dim=100, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "  mse = backend.mean(backend.square(y_true - y_pred))\n",
        "  l1 = backend.mean(backend.abs(y_pred))\n",
        "  return mse + 0.01 * l1\n",
        "\n",
        "model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "Z7N8W1-hcZRp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11.How can you visualize the structure of a neural network model in Keras?\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(128, input_dim=100, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "print(\"MODEL SUMMARY:\")\n",
        "model.summary()\n",
        "\n",
        "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "print(\"Model diagram saved as 'model.png'\")\n",
        "\n",
        "import numpy as np\n",
        "X_train = np.random.rand(100, 100)\n",
        "y_train = tensorflow.keras.utils.to_categorical(np.random.randint(0, 10, 100))\n",
        "\n",
        "log_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tb_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "model.fit(X_train, y_train, epochs=1, callbacks=[tb_callback])\n",
        "\n",
        "print(\"Run this command in your terminal to view TensorBoard:\")\n",
        "print(f\"tensorboard --logdir={log_dir}\")\n"
      ],
      "metadata": {
        "id": "6_BaEbLXdZpp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}